### Deprecated 

## Fisher tests. Reasons not to include:
# - Low statistical power since it is high computationally expensive, we need to run 14 tests per replication (one per round)

# Paragraph removed from the paper
#Then, to assess whether the real data could have originated from the models, we compare the distributions of actual decisions made by individuals in each round against decisions generated by the model agents (using the posterior predictive for each round), using Fisher's exact tests.

########### MODEL A: Bayesian agents with non-informative prior ############################################

#Old text for model A:
#For each of a total of ten replications of the model and for each round, we run Fisher's exact tests (one per round) to compare these simulation results (14000 data points) to those decisions of people in the experiment with the same number of people, 100 in this case. 80\% of the tests found empirical differences in the comparison of the simulated data to the PGG\_H data and 84\% to the PGG\_H2 data.

## Fisher tests

#Using data from real humans from the paper
counts_todas_rondas<-read.csv("/Users/mariapereda/Dropbox/UPM/investigacion/Mis_trabajos_en_curso/PGG_modelito_v2_bayesiano/paraMiPaper/datos/counts_todas_rondas.csv") #DATA FROM ZENODO https://zenodo.org/record/2590686
dist_agregada_PGG_H<-dcast(counts_todas_rondas[counts_todas_rondas$treatment=="PGG_H",], round ~ contribution, value.var = "COUNT", fun.aggregate=sum)
colnames(dist_agregada_PGG_H)<-c("round","c0","c2","c4","c6","c8","c10")


# Conducting r x t tests (r replications, t rounds)
tests <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tests)[1]<-"round"
colnames(tests)[2]<-"replication"
colnames(tests)[3]<-"pvalue" # We use LLR p-value https://cran.r-project.org/web/packages/XNomial/vignettes/XNomial.html
colnames(tests)[4]<-"different" #p-value < 0.001
indice<-1 #just to store the data
final_round <- 14

for (t in 1:final_round){
  for (r in 1:min(10,max(dataModel1$replicationNumber))){ #10 replications at most to save computational power,
    dist_agregada1 <- dcast(dataModel1[dataModel1$round==t & dataModel1$replicationNumber==r,1:3], round ~ contribution, value.var = "frequency", fun.aggregate=sum)
    colnames(dist_agregada1)<-c("round","c0","c2","c4","c6","c8","c10")
    dist_agregada1<-dist_agregada1*100 #N 100 agents
    dist_agregada1$round <- dist_agregada1$round/100
    # We do not run Chi Square tests because samples are small and so the calculation of p-values may be incorrect
    #chisq.test(x=unlist(dist_agregada_PGG_H[dist_agregada_PGG_H$round==t,-1]),y=unlist(dist_agregada1[dist_agregada1$round==t,-1]))
    #We use Fisher exact test https://cran.r-project.org/web/packages/XNomial/vignettes/XNomial.html
    # p-value>alpha means the data fit the model
    # We need at least one data in each category, so I approximate the p-value ensuring all categories have at least one data
    dist_agregada1<-replace(dist_agregada1, dist_agregada1==0, 1)
    
    tests[indice,]$round <- t
    tests[indice,]$replication <- r
    invisible(capture.output(tests[indice,]$pvalue <- xmulti(unlist(dist_agregada_PGG_H[dist_agregada_PGG_H$round==t,-1]),unlist(dist_agregada1[dist_agregada1$round==t,-1]))[4])) #invisible(capture.output to avoid function printing
    tests[indice,]$different <- as.numeric(tests[indice,]$pvalue < 0.001) #simulation data does not fit experimental results
    indice<- indice+1
  }
}

summarytests <- dcast(tests[,-3], round ~ different, value.var = "different", fun.aggregate=sum)
summarytests<-summarytests[,-2]
colnames(summarytests)<-c('round','different')
summarytests$percentagediff <- summarytests$different / min(10,max(dataModel1$replicationNumber))
summarytests
round(mean(summarytests$percentagediff),2)
round(sd(summarytests$percentagediff),2)

#PPGH_2
dist_agregada_PGG_H2<-dcast(counts_todas_rondas[counts_todas_rondas$treatment=="PGG_H2",], round ~ contribution, value.var = "COUNT", fun.aggregate=sum)
colnames(dist_agregada_PGG_H2)<-c("round","c0","c2","c4","c6","c8","c10")


# Conducting r x t tests (r replications, t rounds)
tests <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(tests)[1]<-"round"
colnames(tests)[2]<-"replication"
colnames(tests)[3]<-"pvalue" # We use LLR p-value https://cran.r-project.org/web/packages/XNomial/vignettes/XNomial.html
colnames(tests)[4]<-"different" #p-value < 0.001
indice<-1 #just to store the data
final_round <- 14

for (t in 1:final_round){
  for (r in 1:min(10,max(dataModel1$replicationNumber))){ #10 replications at most to save computational power,
    dist_agregada1 <- dcast(dataModel1[dataModel1$round==t & dataModel1$replicationNumber==r,1:3], round ~ contribution, value.var = "frequency", fun.aggregate=sum)
    colnames(dist_agregada1)<-c("round","c0","c2","c4","c6","c8","c10")
    dist_agregada1<-dist_agregada1*100 #N 100 agents
    dist_agregada1$round <- dist_agregada1$round/100
    # We do not run Chi Square tests because samples are small and so the calculation of p-values may be incorrect
    #chisq.test(x=unlist(dist_agregada_PGG_H[dist_agregada_PGG_H$round==t,-1]),y=unlist(dist_agregada1[dist_agregada1$round==t,-1]))
    #We use Fisher exact test https://cran.r-project.org/web/packages/XNomial/vignettes/XNomial.html
    # We need at least one data in each category, so I approximate the p-value ensuring al categories have at least one data
    dist_agregada1<-replace(dist_agregada1, dist_agregada1==0, 1)
    
    tests[indice,]$round <- t
    tests[indice,]$replication <- r
    invisible(capture.output(tests[indice,]$pvalue <- xmulti(unlist(dist_agregada_PGG_H2[dist_agregada_PGG_H2$round==t,-1]),unlist(dist_agregada1[dist_agregada1$round==t,-1]))[4])) #invisible(capture.output to avoid function printing
    tests[indice,]$different <- as.numeric(tests[indice,]$pvalue < 0.001)
    indice<- indice+1
  }
}

summarytests <- dcast(tests[,-3], round ~ different, value.var = "different", fun.aggregate=sum)
summarytests<-summarytests[,-2]
colnames(summarytests)<-c('round','different')
summarytests$percentagediff <- summarytests$different / min(10,max(dataModel1$replicationNumber))
summarytests
round(mean(summarytests$percentagediff),2)
round(sd(summarytests$percentagediff),2)


########### MODEL B: Bayesian agents with data-based informative prior ############################################

# All texts with temporal series clustering. The values are not updated. JUST KEEP THE TEXT IN CASE I NEED TO RE DO IT, AND I NEED TO WRITE IT IN ENGLISH AGAIN
# For each of a total of ten replications of the model for 100 agents and for each round, we run Fisher exact tests to compare the simulation results per round to those decisions of people in the experiment with the same number of persons. Now we compare people data from rounds 2-14 and simulated data from rounds 1-13, since the model is fed with real data from round 1. $94\%$ (SD=0.22) of the tests found empirical differences in the comparison of the simulated data with the PGG\_H data and $90\%$ (SD = 0.25) with the PGG\_H2 data. If we conduct the same analysis, but compare the simulation data with the experimental data from a PGG with N=100, in which individuals only receive information about the group's average donation in each round, we find that $92\%$ (SD=0.07) of the tests found empirical differences.
#If we compare the simulation results with those of people in a PGG in which the are only informed about the mean of the distribution, we found 61\% (SD=0.33) similarity on average, with 80-90\% similarity for rounds 5 to 11 (as if the data-prior works similar as informing of the mean).




########### MODEL C:
#For each of the ten model replications involving 100 agents and each round, Fisher exact tests were executed to contrast the simulation results with the decisions made by individuals in the experiment with an equivalent number of participants. To make the comparison, we considered data from rounds 2 to 14 for people and rounds 1 to 13 for simulations, as the model was initialized with real data from round 1. Notably, in the case of PGG\_H data, $93\%$ (SD=0.22) of the tests revealed empirical disparities in the comparison of the simulated data. For PGG\_H2 data, $88\%$ (SD=0.24) of the tests showed similar discrepancies. When we replicate this analysis, comparing the simulation data with experimental data from a PGG involving N=100, where participants receive solely the group's average donation information each round, we observe that $82\%$ (SD=0.14) of the tests reveal significant empirical disparities.
# If we compare the simulation results with those of people in a PGG in which the are only informed about the mean of the distribution, we found 61\% (SD=0.33) similarity on average, with 80-90\% similarity for rounds 5 to 11 (as if the data-prior works similar as informing of the mean).



########### MODEL D:
#For each of the ten model replications and every round, Fisher's exact tests were conducted (one test per round) to compare the simulation results (a total of 14,000 data points) with the decisions made by individuals in the experiment, involving 100 participants. In this scenario, 92\% (SD=0.17) of the tests revealed empirical disparities in comparing the simulated data with the PGG\_H data, 84\% (SD=0.15) in the case of the PGG\_H2 data, and 82\% (SD=0.27) in the case of traditional PGG data (as in the previous model).